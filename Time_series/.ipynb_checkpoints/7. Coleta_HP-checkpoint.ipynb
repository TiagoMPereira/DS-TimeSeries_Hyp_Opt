{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC7Vl4lIDsK-"
   },
   "source": [
    "Este notebook tem por finalidade realizar a coleta de hiperparâmetros de forma aleatória para produzir uma base de estudos, afim de definir quais hiperparâmetros deverão ser utilizados de forma eficiente na otimização\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysFd-Yjjy_8j"
   },
   "outputs": [],
   "source": [
    "#bibliotecas padrão\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files\n",
    "\n",
    "#Importando modelos\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "#Importando bibliotecas de otimização\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1gkAhBJzqKu"
   },
   "outputs": [],
   "source": [
    "#Recolhendo dados do facebook e preparando dataset\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "\n",
    "end = dt.datetime(2020, 6, 1)\n",
    "start = dt.datetime(2019, 1, 1)\n",
    "\n",
    "df = web.DataReader(\"FB\", 'yahoo', start, end)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns=['Open','Date','High','Low','Volume','Adj Close'])\n",
    "df = df.rename(columns={'Close': 'Close 0'})\n",
    "\n",
    "def window (df, w):\n",
    "    for i in range(1,w):\n",
    "        df['Close '+str(i)] = df['Close '+str(i-1)].shift(1)\n",
    "    return df\n",
    "        \n",
    "df = window(df,5)\n",
    "df = df.rename(columns={'Close 0': 'Target'})\n",
    "df.dropna(inplace=True)\n",
    "#Separando dados de treino e teste\n",
    "X = df.loc[:, ['Close 1','Close 2','Close 3','Close 4']]\n",
    "y = df.loc[:, 'Target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_m4hYWezn1m"
   },
   "outputs": [],
   "source": [
    "def hp(modelo):\n",
    "      \n",
    "    hp = {}\n",
    "    \n",
    "    #Verificar qual modelo está sendo usado\n",
    "    if modelo=='SVM':\n",
    "\n",
    "      C = (np.arange(10, 100, 5)/100).tolist()\n",
    "\n",
    "      epsilon = []\n",
    "      for i in range (1,5,2):\n",
    "          for j in range (2,10,2):\n",
    "              epsilon.append(j/np.power(10,i))\n",
    "      coef0 = (np.arange(1, 10)/100).tolist()\n",
    "    \n",
    "      hp = [\n",
    "            {\n",
    "                'kernel':['linear'], 'C':C, 'epsilon':epsilon },\n",
    "            {\n",
    "                'kernel':['rbf'], 'gamma': ['scale', 'auto'], 'C':C, 'epsilon':epsilon },\n",
    "            {\n",
    "                'kernel':['sigmoid'], 'gamma': ['scale', 'auto'], 'C':C, 'epsilon':epsilon, 'coef0': coef0 }\n",
    "      ]\n",
    "\n",
    "    elif modelo == 'KNN':\n",
    "\n",
    "      n_n = (np.arange(1,19,2)).tolist()\n",
    "      weights = ['uniform','distance']\n",
    "      p = [1,2]\n",
    "\n",
    "      hp = [\n",
    "            {\n",
    "                'n_neighbors':n_n, 'weights':weights, 'algorithm':['auto','brute'], 'p':p },\n",
    "            {\n",
    "                'n_neighbors':n_n, 'weights':weights, 'algorithm':['ball_tree','kd_tree'], 'p':p, 'leaf_size':(np.arange(1,150,5)).tolist()}\n",
    "      ]\n",
    "    \n",
    "    elif modelo == 'DecisionTree':\n",
    "\n",
    "      min_s_split = (np.arange(2,21,3)).tolist()\n",
    "      max_feat = (np.arange(2,10)/10).tolist()\n",
    "      criterion = ['mse','friedman_mse','mae']\n",
    "      min_s_leaf = (np.arange(1,5)/10).tolist()\n",
    "      spliter = ['best','random']\n",
    "\n",
    "      hp = [\n",
    "            {\n",
    "                'min_samples_split':min_s_split, 'max_features':max_feat, 'max_depth':(np.arange(10,100,10)).tolist(), 'criterion':criterion, \n",
    "                 'splitter':spliter, 'min_samples_leaf': min_s_leaf}\n",
    "      ]\n",
    "    \n",
    "    elif modelo == 'RandomForest':\n",
    "\n",
    "      n_est = (np.arange(50,600,75)).tolist()\n",
    "      #min_s_split = (np.arange(2,20,5)).tolist()\n",
    "      min_s_split = [2,8,14,20]\n",
    "      #max_feat = (np.arange(2,10,2)/10).tolist()\n",
    "      max_feat = [0.3,0.5,0.7]\n",
    "      criterion = ['mse','mae']\n",
    "      #min_s_leaf = (np.arange(1,5)/10).tolist()\n",
    "      min_s_leaf = [0.1,0.3,0.5]\n",
    "      #max_samp = (np.arange(10,100,20)/100).tolist()\n",
    "      max_samp = [0.1,0.3,0.5,0.7,0.9]\n",
    "      max_dpt = [10,30,50,70,90]\n",
    "\n",
    "      hp = [{'n_estimators':n_est, 'min_samples_split':min_s_split, 'max_features':max_feat, 'max_depth':max_dpt, \n",
    "               'min_samples_leaf': min_s_leaf, 'max_samples': max_samp, 'criterion':criterion, 'oob_score':[True,False]}\n",
    "      ]\n",
    "\n",
    "    elif modelo == \"AdaBoost\":\n",
    "\n",
    "      n_est = (np.arange(50,500,50)).tolist()\n",
    "      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\n",
    "      loss = ['linear','exponential','square']\n",
    "\n",
    "      hp = {'n_estimators':n_est, 'learning_rate': lr, 'loss':loss}\n",
    "\n",
    "    elif modelo == \"GradientBoosting\":\n",
    "\n",
    "      n_est = (np.arange(50,500,50)).tolist()\n",
    "      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\n",
    "      #min_s_split = (np.arange(2,20,5)).tolist()\n",
    "      min_s_split = [2,8,14,20]\n",
    "      #max_feat = (np.arange(2,10,2)/10).tolist()\n",
    "      max_feat = [0.3,0.5,0.7]\n",
    "      criterion = ['mse','friedman_mse','mae']\n",
    "      #min_s_leaf = (np.arange(1,5)/10).tolist()\n",
    "      min_s_leaf = [0.1,0.3,0.5]\n",
    "      #max_dpt = (np.arange(10,100,10)).tolist()\n",
    "      max_dpt = [10,30,50,70,90]\n",
    "      loss = ['ls', 'lad', 'huber', 'quantile']\n",
    "      sub = (np.arange(2,10,2)/10).tolist()\n",
    "\n",
    "      hp = [\n",
    "            {\n",
    "             'n_estimators':n_est, 'learning_rate':lr, 'min_samples_split': min_s_split, 'criterion': criterion, 'min_samples_leaf': min_s_leaf,\n",
    "             'max_depth':max_dpt, 'subsample':sub, 'max_features': max_feat, 'loss': ['ls', 'lad'] },\n",
    "            {\n",
    "             'n_estimators':n_est, 'learning_rate':lr, 'min_samples_split': min_s_split, 'criterion': criterion, 'min_samples_leaf': min_s_leaf,\n",
    "             'max_depth':max_dpt, 'subsample':sub, 'max_features': max_feat, 'loss': ['huber', 'quantile'], 'alpha': (np.arange(5,100,7)/100).tolist()}\n",
    "      ]\n",
    "\n",
    "    elif modelo == \"XGBoost\":\n",
    "      n_est = (np.arange(50,500,50)).tolist()\n",
    "      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\n",
    "      sub = (np.arange(2,10,2)/10).tolist()\n",
    "      gamma = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\n",
    "      cbt = (np.arange(2,10,2)/10).tolist()\n",
    "      bst = ['gbtree','gblinear','dart']\n",
    "\n",
    "      hp = {\n",
    "          'n_estimators':n_est, 'learning_rate':lr,'subsample':sub,'gamma':gamma,'colsample_bytree':cbt, 'booster':bst\n",
    "      }\n",
    "\n",
    "\n",
    "\n",
    "    return hp\n",
    "        \n",
    "\n",
    "#Função para Grid Search\n",
    "def HP_Search(modelo):\n",
    "    \n",
    "    ini = time.time()\n",
    "    \n",
    "    #Receber conjunto de hiperparâmetros\n",
    "    hip_space = hp(modelo)\n",
    "    print(hip_space)\n",
    "\n",
    "    n_iter = 1500\n",
    "    \n",
    "    #Verificar qual modelo está sendo usado\n",
    "    if modelo=='SVM':\n",
    "      print('SVM')\n",
    "      model = RandomizedSearchCV(SVR(), hip_space, n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"KNN\":\n",
    "      print(\"KNN\")\n",
    "      model = RandomizedSearchCV(KNR(), hip_space, n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"DecisionTree\":\n",
    "      print(\"DT\")\n",
    "      model = RandomizedSearchCV(DecisionTreeRegressor(), hip_space, n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"RandomForest\":\n",
    "      print(\"rf\")\n",
    "      model = RandomizedSearchCV(RandomForestRegressor(), hip_space, n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"AdaBoost\":\n",
    "      print(\"adb\")\n",
    "      model = RandomizedSearchCV(AdaBoostRegressor(), hip_space,  n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"GradientBoosting\":\n",
    "      print(\"gdb\")\n",
    "      model = RandomizedSearchCV(GradientBoostingRegressor(), hip_space, n_iter=n_iter, verbose=3)\n",
    "    elif modelo == \"XGBoost\":\n",
    "      print(\"xgb\")\n",
    "      model = RandomizedSearchCV(XGBRegressor(), hip_space, n_iter=n_iter, verbose=3)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(end-ini)\n",
    "    return model\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vjUhFBnzw6w"
   },
   "outputs": [],
   "source": [
    "modelos = ['SVM','KNN','DecisionTree','RandomForest','AdaBoost','GradientBoosting','XGBoost']\n",
    "\n",
    "for m in modelos:\n",
    "  mod = HP_Search(m)\n",
    "  df_md = pd.DataFrame.from_dict(mod.cv_results_)\n",
    "  nome = m+\"_HP.csv\"\n",
    "  df_md.to_csv(nome)\n",
    "  files.download(nome) \n",
    "  print(nome + \" baixado com sucesso!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exmECnuMmt2f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqtw3+90EKkvu54SQmrCTu",
   "collapsed_sections": [],
   "name": "Coleta_HP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
