{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando keras tuner para modelo de classificação\n",
    "https://www.tensorflow.org/tutorials/keras/keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definindo modelo\n",
    "#def model_builder(hp):\n",
    "#    model = keras.Sequential()\n",
    "#    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "#\n",
    "#    # Selecionando número de unidades na primeira camada, entre 32 e 512, ao passo de 32\n",
    "#    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "#    model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "#    model.add(keras.layers.Dense(10))\n",
    "#\n",
    "#    # Selecionando valor do learning_rate entre 0,01 , 0,001 , 0,0001\n",
    "#    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "#\n",
    "#    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "#                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "#                metrics = ['accuracy'])\n",
    "#\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sintonizadores disponíveis\n",
    "RandomSearch -> \n",
    "\n",
    "Hyperband ->\n",
    "\n",
    "BayesianOptimization ->\n",
    "\n",
    "Sklearn ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner_hpb = kt.Hyperband(model_builder,\n",
    "#                     objective = 'val_accuracy', \n",
    "#                     max_epochs = 10,\n",
    "#                     factor = 3,\n",
    "#                     directory = 'my_dir',\n",
    "#                     project_name = 'intro_to_kt')\n",
    "#\n",
    "#tuner_bay = kt.BayesianOptimization(model_builder, \n",
    "#                                    objective = 'val_accuracy', \n",
    "#                                    max_trials = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner_hpb.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))\n",
    "\n",
    "## Get the optimal hyperparameters\n",
    "#best_hps = tuner_hpb.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "#print(f\"\"\"\n",
    "#The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "#layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "#is {best_hps.get('learning_rate')}.\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner_bay.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "#best_hps = tuner_bay.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "#print(f\"\"\"\n",
    "#The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "#layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "#is {best_hps.get('learning_rate')}.\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando dependências\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recolhendo dados do facebook e preparando dataset\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "\n",
    "end = dt.datetime(2020, 6, 1)\n",
    "start = dt.datetime(2019, 1, 1)\n",
    "\n",
    "df = web.DataReader(\"FB\", 'yahoo', start, end)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns=['Open','Date','High','Low','Volume','Adj Close'])\n",
    "df = df.rename(columns={'Close': 'Close 0'})\n",
    "\n",
    "def window (df, w):\n",
    "    for i in range(1,w):\n",
    "        df['Close '+str(i)] = df['Close '+str(i-1)].shift(1)\n",
    "    return df\n",
    "        \n",
    "df = window(df,5)\n",
    "df = df.rename(columns={'Close 0': 'Target'})\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Separando dados de treino e teste\n",
    "X = df.loc[:, ['Close 1','Close 2','Close 3','Close 4']]\n",
    "y = df.loc[:, 'Target']\n",
    "\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder__(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Selecionando número de unidades na primeira camada, entre 32 e 512, ao passo de 32\n",
    "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "    model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(1, activation = 'relu'))\n",
    "\n",
    "    # Selecionando valor do learning_rate entre 0,01 , 0,001 , 0,0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "__tuner_hpb = kt.Hyperband(model_builder,\n",
    "                     objective = 'loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                          overwrite=True)\n",
    "\n",
    "__tuner_bay = kt.BayesianOptimization(model_builder, \n",
    "                                    objective = 'loss', \n",
    "                                    max_trials = 10)\n",
    "\n",
    "tuner_rd = kt.RandomSearch(model_builder, \n",
    "                           objective = 'loss', \n",
    "                           max_trials = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 288 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__tuner_hpb.search(X_train, y_train, epochs = 10, validation_data = (X_test, y_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = __tuner_hpb.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "loss: 35211.0859375\n",
      "\n",
      "Best loss So Far: 30.458227157592773\n",
      "Total elapsed time: 00h 00m 40s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 512 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__tuner_bay.search(X_train, y_train, epochs = 10, validation_data = (X_test, y_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = __tuner_bay.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 04s]\n",
      "loss: 30.593875885009766\n",
      "\n",
      "Best loss So Far: 27.611289978027344\n",
      "Total elapsed time: 00h 00m 41s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 64 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner_rd.search(X_train, y_train, epochs = 10, validation_data = (X_test, y_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner_rd.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
