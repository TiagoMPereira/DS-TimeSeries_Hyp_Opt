{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coleta_HP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqtw3+90EKkvu54SQmrCTu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VC7Vl4lIDsK-"},"source":["Este notebook tem por finalidade realizar a coleta de hiperparâmetros de forma aleatória para produzir uma base de estudos, afim de definir quais hiperparâmetros deverão ser utilizados de forma eficiente na otimização\r\n","\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"ysFd-Yjjy_8j"},"source":["#bibliotecas padrão\r\n","import numpy as np\r\n","import time\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","from google.colab import files\r\n","\r\n","#Importando modelos\r\n","from sklearn.svm import SVR\r\n","from sklearn.neighbors import KNeighborsRegressor as KNR\r\n","from sklearn.tree import DecisionTreeRegressor\r\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\r\n","from xgboost import XGBRegressor \r\n","\r\n","#Importando bibliotecas de otimização\r\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1gkAhBJzqKu"},"source":["#Recolhendo dados do facebook e preparando dataset\r\n","\r\n","import pandas_datareader.data as web\r\n","import datetime as dt\r\n","\r\n","end = dt.datetime(2020, 6, 1)\r\n","start = dt.datetime(2019, 1, 1)\r\n","\r\n","df = web.DataReader(\"FB\", 'yahoo', start, end)\r\n","\r\n","df = df.reset_index()\r\n","df = df.drop(columns=['Open','Date','High','Low','Volume','Adj Close'])\r\n","df = df.rename(columns={'Close': 'Close 0'})\r\n","\r\n","def window (df, w):\r\n","    for i in range(1,w):\r\n","        df['Close '+str(i)] = df['Close '+str(i-1)].shift(1)\r\n","    return df\r\n","        \r\n","df = window(df,5)\r\n","df = df.rename(columns={'Close 0': 'Target'})\r\n","df.dropna(inplace=True)\r\n","#Separando dados de treino e teste\r\n","X = df.loc[:, ['Close 1','Close 2','Close 3','Close 4']]\r\n","y = df.loc[:, 'Target'].tolist()\r\n","\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_m4hYWezn1m"},"source":["def hp(modelo):\r\n","      \r\n","    hp = {}\r\n","    \r\n","    #Verificar qual modelo está sendo usado\r\n","    if modelo=='SVM':\r\n","\r\n","      C = (np.arange(10, 100, 5)/100).tolist()\r\n","\r\n","      epsilon = []\r\n","      for i in range (1,5,2):\r\n","          for j in range (2,10,2):\r\n","              epsilon.append(j/np.power(10,i))\r\n","      coef0 = (np.arange(1, 10)/100).tolist()\r\n","    \r\n","      hp = [\r\n","            {\r\n","                'kernel':['linear'], 'C':C, 'epsilon':epsilon },\r\n","            {\r\n","                'kernel':['rbf'], 'gamma': ['scale', 'auto'], 'C':C, 'epsilon':epsilon },\r\n","            {\r\n","                'kernel':['sigmoid'], 'gamma': ['scale', 'auto'], 'C':C, 'epsilon':epsilon, 'coef0': coef0 }\r\n","      ]\r\n","\r\n","    elif modelo == 'KNN':\r\n","\r\n","      n_n = (np.arange(1,19,2)).tolist()\r\n","      weights = ['uniform','distance']\r\n","      p = [1,2]\r\n","\r\n","      hp = [\r\n","            {\r\n","                'n_neighbors':n_n, 'weights':weights, 'algorithm':['auto','brute'], 'p':p },\r\n","            {\r\n","                'n_neighbors':n_n, 'weights':weights, 'algorithm':['ball_tree','kd_tree'], 'p':p, 'leaf_size':(np.arange(1,150,5)).tolist()}\r\n","      ]\r\n","    \r\n","    elif modelo == 'DecisionTree':\r\n","\r\n","      min_s_split = (np.arange(2,21,3)).tolist()\r\n","      max_feat = (np.arange(2,10)/10).tolist()\r\n","      criterion = ['mse','friedman_mse','mae']\r\n","      min_s_leaf = (np.arange(1,5)/10).tolist()\r\n","      spliter = ['best','random']\r\n","\r\n","      hp = [\r\n","            {\r\n","                'min_samples_split':min_s_split, 'max_features':max_feat, 'max_depth':(np.arange(10,100,10)).tolist(), 'criterion':criterion, \r\n","                 'splitter':spliter, 'min_samples_leaf': min_s_leaf}\r\n","      ]\r\n","    \r\n","    elif modelo == 'RandomForest':\r\n","\r\n","      n_est = (np.arange(50,600,75)).tolist()\r\n","      #min_s_split = (np.arange(2,20,5)).tolist()\r\n","      min_s_split = [2,8,14,20]\r\n","      #max_feat = (np.arange(2,10,2)/10).tolist()\r\n","      max_feat = [0.3,0.5,0.7]\r\n","      criterion = ['mse','mae']\r\n","      #min_s_leaf = (np.arange(1,5)/10).tolist()\r\n","      min_s_leaf = [0.1,0.3,0.5]\r\n","      #max_samp = (np.arange(10,100,20)/100).tolist()\r\n","      max_samp = [0.1,0.3,0.5,0.7,0.9]\r\n","      max_dpt = [10,30,50,70,90]\r\n","\r\n","      hp = [{'n_estimators':n_est, 'min_samples_split':min_s_split, 'max_features':max_feat, 'max_depth':max_dpt, \r\n","               'min_samples_leaf': min_s_leaf, 'max_samples': max_samp, 'criterion':criterion, 'oob_score':[True,False]}\r\n","      ]\r\n","\r\n","    elif modelo == \"AdaBoost\":\r\n","\r\n","      n_est = (np.arange(50,500,50)).tolist()\r\n","      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\r\n","      loss = ['linear','exponential','square']\r\n","\r\n","      hp = {'n_estimators':n_est, 'learning_rate': lr, 'loss':loss}\r\n","\r\n","    elif modelo == \"GradientBoosting\":\r\n","\r\n","      n_est = (np.arange(50,500,50)).tolist()\r\n","      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\r\n","      #min_s_split = (np.arange(2,20,5)).tolist()\r\n","      min_s_split = [2,8,14,20]\r\n","      #max_feat = (np.arange(2,10,2)/10).tolist()\r\n","      max_feat = [0.3,0.5,0.7]\r\n","      criterion = ['mse','friedman_mse','mae']\r\n","      #min_s_leaf = (np.arange(1,5)/10).tolist()\r\n","      min_s_leaf = [0.1,0.3,0.5]\r\n","      #max_dpt = (np.arange(10,100,10)).tolist()\r\n","      max_dpt = [10,30,50,70,90]\r\n","      loss = ['ls', 'lad', 'huber', 'quantile']\r\n","      sub = (np.arange(2,10,2)/10).tolist()\r\n","\r\n","      hp = [\r\n","            {\r\n","             'n_estimators':n_est, 'learning_rate':lr, 'min_samples_split': min_s_split, 'criterion': criterion, 'min_samples_leaf': min_s_leaf,\r\n","             'max_depth':max_dpt, 'subsample':sub, 'max_features': max_feat, 'loss': ['ls', 'lad'] },\r\n","            {\r\n","             'n_estimators':n_est, 'learning_rate':lr, 'min_samples_split': min_s_split, 'criterion': criterion, 'min_samples_leaf': min_s_leaf,\r\n","             'max_depth':max_dpt, 'subsample':sub, 'max_features': max_feat, 'loss': ['huber', 'quantile'], 'alpha': (np.arange(5,100,7)/100).tolist()}\r\n","      ]\r\n","\r\n","    elif modelo == \"XGBoost\":\r\n","      n_est = (np.arange(50,500,50)).tolist()\r\n","      lr = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\r\n","      sub = (np.arange(2,10,2)/10).tolist()\r\n","      gamma = [1e-3, 5e-3, 9e-3, 1e-1, 5e-1, 9e-1, 1e-5, 5e-5, 9e-5]\r\n","      cbt = (np.arange(2,10,2)/10).tolist()\r\n","      bst = ['gbtree','gblinear','dart']\r\n","\r\n","      hp = {\r\n","          'n_estimators':n_est, 'learning_rate':lr,'subsample':sub,'gamma':gamma,'colsample_bytree':cbt, 'booster':bst\r\n","      }\r\n","\r\n","\r\n","\r\n","    return hp\r\n","        \r\n","\r\n","#Função para Grid Search\r\n","def HP_Search(modelo):\r\n","    \r\n","    ini = time.time()\r\n","    \r\n","    #Receber conjunto de hiperparâmetros\r\n","    hip_space = hp(modelo)\r\n","    print(hip_space)\r\n","\r\n","    n_iter = 1500\r\n","    \r\n","    #Verificar qual modelo está sendo usado\r\n","    if modelo=='SVM':\r\n","      print('SVM')\r\n","      model = RandomizedSearchCV(SVR(), hip_space, n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"KNN\":\r\n","      print(\"KNN\")\r\n","      model = RandomizedSearchCV(KNR(), hip_space, n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"DecisionTree\":\r\n","      print(\"DT\")\r\n","      model = RandomizedSearchCV(DecisionTreeRegressor(), hip_space, n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"RandomForest\":\r\n","      print(\"rf\")\r\n","      model = RandomizedSearchCV(RandomForestRegressor(), hip_space, n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"AdaBoost\":\r\n","      print(\"adb\")\r\n","      model = RandomizedSearchCV(AdaBoostRegressor(), hip_space,  n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"GradientBoosting\":\r\n","      print(\"gdb\")\r\n","      model = RandomizedSearchCV(GradientBoostingRegressor(), hip_space, n_iter=n_iter, verbose=3)\r\n","    elif modelo == \"XGBoost\":\r\n","      print(\"xgb\")\r\n","      model = RandomizedSearchCV(XGBRegressor(), hip_space, n_iter=n_iter, verbose=3)\r\n","\r\n","    model.fit(X_train, y_train)\r\n","    end = time.time()\r\n","    print(end-ini)\r\n","    return model\r\n","    \r\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vjUhFBnzw6w"},"source":["modelos = ['SVM','KNN','DecisionTree','RandomForest','AdaBoost','GradientBoosting','XGBoost']\r\n","\r\n","for m in modelos:\r\n","  mod = HP_Search(m)\r\n","  df_md = pd.DataFrame.from_dict(mod.cv_results_)\r\n","  nome = m+\"_HP.csv\"\r\n","  df_md.to_csv(nome)\r\n","  files.download(nome) \r\n","  print(nome + \" baixado com sucesso!!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exmECnuMmt2f"},"source":[""],"execution_count":null,"outputs":[]}]}